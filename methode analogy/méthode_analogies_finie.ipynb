{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, on essaye de mettre en place une méthode d'analogie qui exhibe les stéréotypes de genre présentée dans l'article écrit par Bolukbasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les tâches d'analogie standard, on nous donne trois mots, par exemple \"he\", \"she\", \"king\", et on cherche le 4ᵉ mot pour résoudre la relation \"he\" vers \"king\" est comme \"she\" vers \"x\". Ici, nous modifions la tâche d'analogie de manière à ce qu'avec deux mots donnés, par exemple \"he\" et \"she\", nous voulons générer une paire de mots, \"x\" et \"y\", telle que la relation \"he\" vers \"x\" soit similaire à la relation \"she\" vers \"y\" et constitue une bonne analogie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette modification nous permet de générer systématiquement des paires de mots que l'embedding considère comme analogues à \"he\" et \"she\" (ou à toute autre paire de mots de référence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrée dans notre générateur d'analogies est une paire de mots de départ (a, b) déterminant une direction de départ vect(a) − vect(b) correspondant à la différence normalisée entre les deux mots de départ. Dans la tâche ci-dessous, nous utilisons (a, b) = (she, he). Nous évaluons ensuite toutes les paires de mots x, y en utilisant la métrique suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(a,b)(x, y) = \\begin{cases} \n",
    "\\cos(\\mathbf{\\tilde{a} - \\tilde{b}}, \\mathbf{\\tilde{x} - \\tilde{y}}) & \\text{if } \\|\\mathbf{\\tilde{x} - \\tilde{y}}\\| \\leq \\delta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où δ est un seuil de similarité. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intuition de la métrique de notation est que nous souhaitons qu'une paire d'analogies soit proche et parallèle à la direction de départ, tandis que les deux mots ne doivent pas être trop éloignés pour être sémantiquement cohérents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre δ établit le seuil de similarité sémantique. En pratique, il est choisi égal à 1 dans l'article mais **à étudier**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est parti !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824871301651001),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('teenage_girl', 0.6147903203964233),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('suspected_purse_snatcher', 0.571636438369751),\n",
       " ('robber', 0.5585119128227234),\n",
       " ('Robbery_suspect', 0.5584409832954407),\n",
       " ('teen_ager', 0.5549196600914001),\n",
       " ('men', 0.5489763021469116)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etape 1 :** traitement des données. \\\n",
    "On traite le word embedding selon la même procédure que celle présentée dans l'article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.unit_normalize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.sort_by_descending_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_chaine(chaine) :\n",
    "    if chaine.islower() and chaine.isalpha() and len(chaine)<20 :\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['in', 'for', 'that', 'is', 'on', 'with', 'said', 'was', 'the', 'at'], 24065)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sorted = first_model.index_to_key[:50000]\n",
    "words_sorted = [word for word in words_sorted if verifier_chaine(word)]\n",
    "words_sorted[:10], len(words_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = first_model.vector_size\n",
    "model = models.keyedvectors.KeyedVectors(vector_size, count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_sorted :\n",
    "    model[word] = first_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824870109558105),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('robber', 0.5585119128227234),\n",
       " ('men', 0.5489763021469116),\n",
       " ('guy', 0.5420035719871521),\n",
       " ('person', 0.5342026352882385),\n",
       " ('gentleman', 0.5337990522384644),\n",
       " ('motorcyclist', 0.5336882472038269)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a fini la première étape de traitement des données. Question reste en suspens : pourquoi je n'ai pas le même nombre de mots dans mon word embedding filtré que dans l'article ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on passe à l'étape 2. Codons l'algorithme d'analogies de l'article ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs de mot ont été normalisés, donc calculer la cosinus-similarité revient à calculer le produit scalaire entre deux vecteurs. Ce produit scalaire peut être calculé par la fonction linear kernel : \\\n",
    "La fonction linear_kernel prend en entrée deux matrices X et Y, où chaque ligne de chaque matrice représente un échantillon (donc dans notre cas, un mot) et chaque colonne représente une caractéristique (une des coordonnées du vecteur associé). Elle renvoie une matrice où l'élément à la position (i, j) est le noyau linéaire entre le i-ème échantillon de la première matrice (X) et le j-ème échantillon de la deuxième matrice (Y). Donc dans notre cas, elle renvoie la similarité entre le mot i de la matrice X et le mot j de la matrice Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test1 = np.ones((3,4))\n",
    "print(linear_kernel(test1,test1))\n",
    "#on fait le linear kernel entre deux matrices qui ont 3 mots chacune\n",
    "#donc on calcule la similarité entre 9 mots différents\n",
    "#la matrice contient effectivement 9 valeurs\n",
    "print(cosine_similarity(test1,test1))\n",
    "#linear kernel et cosine similarity ne sont pas toujours la même chose\n",
    "#si la matrice n'est pas normalisée, les valeurs ne sont pas les mêmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24065, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on veut une matrice X qui contient les vecteurs de chaque mot\n",
    "#elle a donc nb_words lignes et vector_size colonnes\n",
    "X = model.vectors\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on fait linear_kernel(X,X), la matrice va nous donner la cosinus_similarité de toutes les paires de mots possibles. Ce sera une matrice symétrique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24065, 24065)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = linear_kernel(X)\n",
    "#quand on ne précise pas de deuxième matrice, la fonction calcule la\n",
    "#similarité entre chaque ligne de X et elle-même, donc linear_kernel(X,X)\n",
    "print(cos_sim.shape)\n",
    "cos_sim[1,2] == cos_sim[2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a compris comment fonctionne linear_kernel, appliquons le dans notre algoroithme d'analogies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on trouve les vecteurs associés à she et he \n",
    "vect_she = model.get_vector('she')\n",
    "vect_he = model.get_vector('he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nos vecteurs sont normalisées, on a pour x,y des vecteurs quelconques du word embedding et a,b des vecteurs fixes (qui ici joueront le rôle de he,she) : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\cos(\\mathbf{a}-\\mathbf{b}, \\mathbf{x}-\\mathbf{y}) = (\\mathbf{a}-\\mathbf{b})^T \\cdot (\\mathbf{x}-\\mathbf{y}) = \\mathbf{a}^T \\cdot \\mathbf{x} - \\mathbf{b}^T \\cdot \\mathbf{x} - \\mathbf{a}^T \\cdot \\mathbf{y} + \\mathbf{b}^T \\cdot \\mathbf{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je veux calculer une matrice de similarité mat_sim telle que, en notant i,j les index associés respectivement aux mots x et y, on a : sim_mat(i,j) = cos(a-b,x-y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_visée = (vect_she - vect_he)/np.linalg.norm(vect_she - vect_he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1 = linear_kernel((diff_visée).reshape(1,-1), X) \n",
    "#me donne une ligne et 24065 colonnes\n",
    "#chaque colonne correspond à la cos_sim entre un mot et le vect diff she-he\n",
    "#attention, il faut normaliser la diff pour pouvoir utilsier linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros((nb_words,nb_words))\n",
    "\n",
    "xmat_1 = np.array(mat_1[0,:])\n",
    "\n",
    "sim_mat = np.tile(xmat_1[np.newaxis,:], (nb_words,1))-np.tile(xmat_1[:,np.newaxis], (1,nb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24065, 24065)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n",
      "months\n",
      "-0.037064265\n",
      "0.04987895\n",
      "0.08694322\n"
     ]
    }
   ],
   "source": [
    "dir = vect_she - vect_he\n",
    "n = 9\n",
    "p = 200\n",
    "print(model.index_to_key[n])\n",
    "print(model.index_to_key[p])\n",
    "diff = model[n] - model[p]\n",
    "sim = cosine_similarity(dir.reshape(1, -1), diff.reshape(1, -1))[0,0]\n",
    "print(sim)\n",
    "print(sim_mat[n,p])\n",
    "print(sim_mat[n,p]-sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe une légère différence dans les deux calculs de la cos_similarité mais leur différence semble toujours du même signe. On peut espérer que cela n'influe pas trop sur l'ordre des valeurs de coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions si les résultats trouvés sont les mêmes que ceux de l'article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a une matrice qui nous donne les similarités, on va chercher les paires de mots associées aux plus gros coefficients, et qui vérifient que norm(x-y) < 1 (cf. condition dans l'article). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode initiale : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_coefs = -np.ones(10)\n",
    "# indices_best = [0]*10\n",
    "# for i in range(nb_words):\n",
    "#     if i%1000 == 0 :\n",
    "#         print(i, 'mots traités sur', nb_words)\n",
    "#     for j in range(i+1, nb_words): \n",
    "#         if sim_mat[i,j] > np.min(best_coefs):\n",
    "#             if np.linalg.norm(model[i]-model[j]) < 1 : \n",
    "#                 index = np.argmin(best_coefs)\n",
    "#                 best_coefs[index] = sim_mat[i,j] \n",
    "#                 indices_best[index] = (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus efficace : faire un tri des coefs puis parmi les coefs, regardez ceux qui vérifient la condition sur la norme. Pour le tri des coefs, on peut faire un tri partiel : commencer par trier les 10 000 premiers coefs et voir si parmi eux, on peut pas en avoir 10 qui vérifient la condition de norme. Regarder partial sort dans la doc --> fonction argpartition : je commence à trier les 1000 premiers, je fais mon test et si j'ai déjà les 10, je m'arrête. Algo itératif. Il faut mettre la matrice sim_mat à plat, regarder les arguments donnés par argpartition, utiliser unravel pour retrouver le i,j qui correspond à cet indice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que sim_mat soit votre matrice de similarité et model votre array de vecteurs\n",
    "best_coefs = -np.ones(100)\n",
    "indices_best = [0] * 100\n",
    "\n",
    "# Utilisez argpartition pour obtenir les indices triés des 1000 plus grands coefficients de toute la matrice\n",
    "sorted_indices = np.argpartition(-sim_mat, 1000000, axis=None)[:1000000]\n",
    "\n",
    "# Utilisez unravel_index pour obtenir les indices dans la matrice d'origine\n",
    "indices_in_sim_mat = np.unravel_index(sorted_indices, sim_mat.shape)\n",
    "\n",
    "# Parcourez les indices et mettez à jour les listes best_coefs et indices_best\n",
    "for i, j in zip(*indices_in_sim_mat):\n",
    "    if sim_mat[i, j] > np.min(best_coefs):\n",
    "        if np.linalg.norm(model[i] - model[j]) < 1:\n",
    "            index = np.argmin(best_coefs)\n",
    "            best_coefs[index] = sim_mat[i, j]\n",
    "            indices_best[index] = (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37701178 0.61596274 0.48028737 0.37364566 0.4798581  0.51472658\n",
      " 0.48976302 0.3735052  0.39002678 0.37368891 0.41738576 0.56172156\n",
      " 0.46421447 0.48796439 0.38053083 0.44171065 0.42500311 0.44685441\n",
      " 0.41985148 0.44541574 0.45323342 0.40022433 0.44366646 0.37632149\n",
      " 0.40132248 0.39969683 0.40044248 0.39130366 0.44063488 0.39350483\n",
      " 0.3793433  0.41783339 0.38414389 0.4213554  0.47627163 0.44463554\n",
      " 0.44506505 0.39175555 0.46092471 0.42772996 0.46319872 0.40993339\n",
      " 0.38716882 0.39778835 0.40692717 0.37255162 0.37229171 0.48022708\n",
      " 0.38284594 0.38280618 0.45015118 0.40670905 0.45971811 0.45190045\n",
      " 0.37843674 0.38927019 0.42116946 0.37548172 0.38947111 0.41760045\n",
      " 0.47507542 0.37999514 0.38019988 0.37497556 0.44278529 0.40464377\n",
      " 0.37855807 0.3820484  0.73908675 0.54357237 0.45801437 0.39602003\n",
      " 0.39527562 0.87977844 0.38095343 0.37348789 0.3823114  0.37600511\n",
      " 0.38411486 0.37668198 0.54205835 0.37759784 0.42082185 0.37737972\n",
      " 0.43038878 0.42257112 0.39799502 0.46753994 0.43175265 0.43746403\n",
      " 0.39769411 0.77464449 0.43321362 0.44358325 0.45140088 0.39839184\n",
      " 0.44183397 0.40703353 0.42123631 0.39860994]\n",
      "[(5954, 20751), (3513, 13393), (9559, 4231), (7726, 4785), (22583, 16295), (206, 527), (206, 1165), (11002, 22816), (11002, 16197), (8259, 4549), (9660, 18027), (4719, 18301), (4616, 11231), (4616, 7245), (4728, 1110), (1653, 2346), (2156, 11694), (20880, 18506), (2156, 3089), (7544, 706), (7544, 1001), (7544, 1070), (7544, 1961), (7544, 3886), (757, 1001), (757, 3135), (7544, 8870), (7544, 9993), (2528, 19740), (757, 706), (7544, 19019), (2528, 3135), (7544, 20951), (2528, 1165), (11994, 18567), (596, 4881), (596, 5215), (757, 1961), (16239, 23070), (4426, 23065), (10862, 18506), (10862, 17769), (3081, 19740), (1414, 9993), (1414, 8870), (1414, 7770), (1414, 4891), (1414, 4462), (1414, 4389), (1414, 3886), (1414, 1961), (1414, 1070), (1414, 1001), (1414, 706), (438, 8128), (14364, 13393), (4560, 19534), (18810, 11537), (5954, 9993), (12936, 20583), (12936, 7245), (11545, 22928), (1276, 527), (15100, 20448), (109, 57), (645, 1716), (8338, 10735), (11227, 3089), (665, 2346), (18438, 19534), (665, 16333), (987, 2346), (7149, 8128), (18, 62), (753, 2346), (689, 1001), (5954, 20951), (355, 317), (10083, 8109), (5377, 14502), (878, 8128), (5377, 8870), (5377, 1961), (5377, 1070), (5377, 1001), (5377, 706), (17543, 4462), (2919, 4462), (15890, 19534), (2919, 1961), (5954, 14502), (21, 57), (16834, 18567), (5954, 706), (5954, 1001), (5954, 1070), (5954, 1961), (6621, 14791), (7658, 18301), (5954, 8870)]\n"
     ]
    }
   ],
   "source": [
    "print(best_coefs)\n",
    "print(indices_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncle\n",
      "aunts\n",
      "\n",
      "hero\n",
      "heroine\n",
      "\n",
      "gentleman\n",
      "lady\n",
      "\n",
      "surgeon\n",
      "nurse\n",
      "\n",
      "fatherhood\n",
      "motherhood\n",
      "\n",
      "man\n",
      "woman\n",
      "\n",
      "man\n",
      "girl\n",
      "\n",
      "beard\n",
      "ponytail\n",
      "\n",
      "beard\n",
      "wig\n",
      "\n",
      "fathers\n",
      "mothers\n",
      "\n",
      "fraternity\n",
      "sorority\n",
      "\n",
      "businessman\n",
      "businesswoman\n",
      "\n",
      "king\n",
      "princess\n",
      "\n",
      "king\n",
      "queen\n",
      "\n",
      "youngsters\n",
      "girls\n",
      "\n",
      "myself\n",
      "herself\n",
      "\n",
      "actor\n",
      "actresses\n",
      "\n",
      "fella\n",
      "gal\n",
      "\n",
      "actor\n",
      "actress\n",
      "\n",
      "nephew\n",
      "mother\n",
      "\n",
      "nephew\n",
      "daughter\n",
      "\n",
      "nephew\n",
      "husband\n",
      "\n",
      "nephew\n",
      "sister\n",
      "\n",
      "nephew\n",
      "daughters\n",
      "\n",
      "father\n",
      "daughter\n",
      "\n",
      "father\n",
      "mom\n",
      "\n",
      "nephew\n",
      "granddaughter\n",
      "\n",
      "nephew\n",
      "niece\n",
      "\n",
      "kid\n",
      "mommy\n",
      "\n",
      "father\n",
      "mother\n",
      "\n",
      "nephew\n",
      "stepdaughter\n",
      "\n",
      "kid\n",
      "mom\n",
      "\n",
      "nephew\n",
      "granddaughters\n",
      "\n",
      "kid\n",
      "girl\n",
      "\n",
      "lad\n",
      "schoolgirl\n",
      "\n",
      "football\n",
      "softball\n",
      "\n",
      "football\n",
      "volleyball\n",
      "\n",
      "father\n",
      "sister\n",
      "\n",
      "patriarch\n",
      "matriarch\n",
      "\n",
      "shirt\n",
      "blouse\n",
      "\n",
      "dude\n",
      "gal\n",
      "\n",
      "dude\n",
      "chick\n",
      "\n",
      "dad\n",
      "mommy\n",
      "\n",
      "brother\n",
      "niece\n",
      "\n",
      "brother\n",
      "granddaughter\n",
      "\n",
      "brother\n",
      "aunt\n",
      "\n",
      "brother\n",
      "grandmother\n",
      "\n",
      "brother\n",
      "sisters\n",
      "\n",
      "brother\n",
      "boyfriend\n",
      "\n",
      "brother\n",
      "daughters\n",
      "\n",
      "brother\n",
      "sister\n",
      "\n",
      "brother\n",
      "husband\n",
      "\n",
      "brother\n",
      "daughter\n",
      "\n",
      "brother\n",
      "mother\n",
      "\n",
      "member\n",
      "chairwoman\n",
      "\n",
      "protagonist\n",
      "heroine\n",
      "\n",
      "tall\n",
      "petite\n",
      "\n",
      "barber\n",
      "salon\n",
      "\n",
      "uncle\n",
      "niece\n",
      "\n",
      "kings\n",
      "queens\n",
      "\n",
      "kings\n",
      "queen\n",
      "\n",
      "councilman\n",
      "councilwoman\n",
      "\n",
      "boy\n",
      "woman\n",
      "\n",
      "conservatism\n",
      "feminism\n",
      "\n",
      "my\n",
      "her\n",
      "\n",
      "spokesman\n",
      "spokeswoman\n",
      "\n",
      "yours\n",
      "hers\n",
      "\n",
      "entertainer\n",
      "actress\n",
      "\n",
      "himself\n",
      "herself\n",
      "\n",
      "lanky\n",
      "petite\n",
      "\n",
      "himself\n",
      "oneself\n",
      "\n",
      "itself\n",
      "herself\n",
      "\n",
      "chaired\n",
      "chairwoman\n",
      "\n",
      "he\n",
      "she\n",
      "\n",
      "themselves\n",
      "herself\n",
      "\n",
      "son\n",
      "daughter\n",
      "\n",
      "uncle\n",
      "granddaughters\n",
      "\n",
      "men\n",
      "women\n",
      "\n",
      "homosexual\n",
      "lesbian\n",
      "\n",
      "grandfather\n",
      "grandma\n",
      "\n",
      "chairman\n",
      "chairwoman\n",
      "\n",
      "grandfather\n",
      "granddaughter\n",
      "\n",
      "grandfather\n",
      "sister\n",
      "\n",
      "grandfather\n",
      "husband\n",
      "\n",
      "grandfather\n",
      "daughter\n",
      "\n",
      "grandfather\n",
      "mother\n",
      "\n",
      "uncles\n",
      "sisters\n",
      "\n",
      "brothers\n",
      "sisters\n",
      "\n",
      "diminutive\n",
      "petite\n",
      "\n",
      "brothers\n",
      "sister\n",
      "\n",
      "uncle\n",
      "grandma\n",
      "\n",
      "his\n",
      "her\n",
      "\n",
      "schoolboy\n",
      "schoolgirl\n",
      "\n",
      "uncle\n",
      "mother\n",
      "\n",
      "uncle\n",
      "daughter\n",
      "\n",
      "uncle\n",
      "husband\n",
      "\n",
      "uncle\n",
      "sister\n",
      "\n",
      "superstar\n",
      "diva\n",
      "\n",
      "entrepreneur\n",
      "businesswoman\n",
      "\n",
      "uncle\n",
      "granddaughter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in indices_best :\n",
    "    analog_she = model.index_to_key[i]\n",
    "    analog_he = model.index_to_key[j]\n",
    "    print(analog_she)\n",
    "    print(analog_he)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
