{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, on essaye de mettre en place une méthode d'analogie qui exhibe les stéréotypes de genre présentée dans l'article écrit par Bolukbasi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les tâches d'analogie standard, on nous donne trois mots, par exemple \"he\", \"she\", \"king\", et on cherche le 4ᵉ mot pour résoudre la relation \"he\" vers \"king\" est comme \"she\" vers \"x\". Ici, nous modifions la tâche d'analogie de manière à ce qu'avec deux mots donnés, par exemple \"he\" et \"she\", nous voulons générer une paire de mots, \"x\" et \"y\", telle que la relation \"he\" vers \"x\" soit similaire à la relation \"she\" vers \"y\" et constitue une bonne analogie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette modification nous permet de générer systématiquement des paires de mots que l'embedding considère comme analogues à \"he\" et \"she\" (ou à toute autre paire de mots de référence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrée dans notre générateur d'analogies est une paire de mots de départ (a, b) déterminant une direction de départ vect(a) − vect(b) correspondant à la différence normalisée entre les deux mots de départ. Dans la tâche ci-dessous, nous utilisons (a, b) = (she, he). Nous évaluons ensuite toutes les paires de mots x, y en utilisant la métrique suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(a,b)(x, y) = \\begin{cases} \n",
    "\\cos(\\mathbf{\\tilde{a} - \\tilde{b}}, \\mathbf{\\tilde{x} - \\tilde{y}}) & \\text{if } \\|\\mathbf{\\tilde{x} - \\tilde{y}}\\| \\leq \\delta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où δ est un seuil de similarité. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intuition de la métrique de notation est que nous souhaitons qu'une paire d'analogies soit proche et parallèle à la direction de départ, tandis que les deux mots ne doivent pas être trop éloignés pour être sémantiquement cohérents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre δ établit le seuil de similarité sémantique. En pratique, il est choisi égal à 1 dans l'article mais **à étudier**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est parti !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeyedVectors est une classe dans la bibliothèque Gensim qui représente des vecteurs de mots (ou \"word vectors\"). Elle est utilisée pour stocker et manipuler des embeddings de mots.Dans Gensim, après avoir chargé un modèle Word2Vec pré-entraîné à partir d'un fichier, le modèle est généralement stocké sous forme d'objet KeyedVectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824871301651001),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('teenage_girl', 0.6147903203964233),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('suspected_purse_snatcher', 0.571636438369751),\n",
       " ('robber', 0.5585119128227234),\n",
       " ('Robbery_suspect', 0.5584409832954407),\n",
       " ('teen_ager', 0.5549196600914001),\n",
       " ('men', 0.5489763021469116)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etape 1 :** traitement des données. \\\n",
    "On traite le word embedding selon la même procédure que celle présentée dans l'article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = first_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.sort_by_descending_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.unit_normalize_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise les word vectors. Cette étape peut être utile pour les raisons suivantes : \n",
    "\n",
    "1. **Comparaison facilitée :** En normalisant les vecteurs, vous pouvez comparer plus facilement la similarité entre les mots en utilisant des mesures telles que la similarité cosinus. La similarité cosinus entre deux vecteurs normalisés est simplement le produit scalaire des vecteurs, ce qui simplifie les calculs.\n",
    "\n",
    "2. **Interprétation plus facile :** Les vecteurs normalisés peuvent rendre les interprétations des relations entre les mots plus directes, car la longueur des vecteurs ne joue pas un rôle majeur dans les comparaisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on fait la sélection de mots suivants. À partir des 50 000 mots les plus fréquents, choisissez uniquement les mots en minuscules (pour éviter une distinction artificielle uniquement dû à la casse) et les phrases de moins de 20 caractères en minuscules. Les mots avec des lettres majuscules, des chiffres ou de la ponctuation doivent être exclus. Après cette étape de filtrage, il doit rester 26,377 mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_chaine(chaine) :\n",
    "    if chaine.islower() and chaine.isalpha() and len(chaine)<20 :\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Gensim, l'attribut `index_to_key` est utilisé pour accéder à la liste des mots (ou clés) dans le modèle `KeyedVectors`. Cet attribut est un dictionnaire qui mappe les indices numériques attribués à chaque mot aux mots eux-mêmes.\n",
    "\n",
    "- **Fonctionnement :** Chaque mot dans le modèle Word2Vec est associé à un index numérique unique. L'attribut `index_to_key` est un dictionnaire où les clés sont ces indices numériques et les valeurs sont les mots correspondants.L'ordre de numérotation correspond à l'ordre dans lequel les mots apparaissent dans le corpus. \n",
    "\n",
    "- **Exemple :** Supposons que `model.index_to_key` renvoie quelque chose comme `{0: 'chat', 1: 'chien', 2: 'oiseau', ...}`. Cela signifie que le mot \"chat\" a l'index 0, le mot \"chien\" a l'index 1, le mot \"oiseau\" a l'index 2, et ainsi de suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['in', 'for', 'that', 'is', 'on', 'with', 'said', 'was', 'the', 'at'], 24065)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sorted = first_model.index_to_key[:50000]\n",
    "words_sorted = [word for word in words_sorted if verifier_chaine(word)]\n",
    "words_sorted[:10], len(words_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24065\n"
     ]
    }
   ],
   "source": [
    "nb_words = len(words_sorted)\n",
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.keyedvectors.KeyedVectors(vector_size, count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_sorted :\n",
    "    model[word] = first_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824870109558105),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('robber', 0.5585119128227234),\n",
       " ('men', 0.5489763021469116),\n",
       " ('guy', 0.5420035719871521),\n",
       " ('person', 0.5342026352882385),\n",
       " ('gentleman', 0.5337990522384644),\n",
       " ('motorcyclist', 0.5336882472038269)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a fini la première étape de traitement des données. Question reste en suspens : pourquoi je n'ai pas le même nombre de mots dans mon word embedding filtré que dans l'article ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on passe à l'étape 2. Codons l'algorithme d'analogies de l'article ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs de mot ont été normalisés, donc calculer la cosinus-similarité revient à calculer le produit scalaire entre deux vecteurs. Ce produit scalaire peut être calculé par la fonction linear kernel : \\\n",
    "La fonction linear_kernel prend en entrée deux matrices X et Y, où chaque ligne de chaque matrice représente un échantillon (donc dans notre cas, un mot) et chaque colonne représente une caractéristique (une des coordonnées du vecteur associé). Elle renvoie une matrice où l'élément à la position (i, j) est le noyau linéaire entre le i-ème échantillon de la première matrice (X) et le j-ème échantillon de la deuxième matrice (Y). Donc dans notre cas, elle renvoie la similarité entre le mot i de la matrice X et le mot j de la matrice Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test1 = np.ones((3,4))\n",
    "print(linear_kernel(test1,test1))\n",
    "#on fait le linear kernel entre deux matrices qui ont 3 mots chacune\n",
    "#donc on calcule la similarité entre 9 mots différents\n",
    "#la matrice contient effectivement 9 valeurs\n",
    "print(cosine_similarity(test1,test1))\n",
    "#linear kernel et cosine similarity ne sont pas toujours la même chose\n",
    "#si la matrice n'est pas normalisée, les valeurs ne sont pas les mêmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24065, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on veut une matrice X qui contient les vecteurs de chaque mot\n",
    "#elle a donc nb_words lignes et vector_size colonnes\n",
    "X = model.vectors\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on fait linear_kernel(X,X), la matrice va nous donner la cosinus_similarité de toutes les paires de mots possibles. Ce sera une matrice symétrique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24065, 24065)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = linear_kernel(X)\n",
    "#quand on ne précise pas de deuxième matrice, la fonction calcule la\n",
    "#similarité entre chaque ligne de X et elle-même, donc linear_kernel(X,X)\n",
    "print(cos_sim.shape)\n",
    "cos_sim[1,2] == cos_sim[2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a compris comment fonctionne linear_kernel, appliquons le dans notre algoroithme d'analogies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on trouve les vecteurs associés à she et he \n",
    "vect_she = model.get_vector('she')\n",
    "vect_he = model.get_vector('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = linear_kernel((vect_she-vect_he).reshape(1,-1), X)  \n",
    "#on calcule la similarité entre le vecteur she-he et tous les autres vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 647. GiB for an array with shape (289550080, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#maintenant, il faut calculer la similarité entre le vecteur she-he et \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#toutes les différentes possibles entre deux mots\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#on va donc faire une matrice de taille nb_words*(nb_words-1)/2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#on crée une matrice de zéros   \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m sim_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnb_words\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnb_words\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#les vecteurs sont de dimension 300\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_words) :\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,nb_words) :\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 647. GiB for an array with shape (289550080, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "#maintenant, il faut calculer la similarité entre le vecteur she-he et \n",
    "#toutes les différentes possibles entre deux mots\n",
    "#on va donc faire une matrice de taille nb_words*(nb_words-1)/2\n",
    "\n",
    "#on crée une matrice de zéros   \n",
    "sim_mat = np.zeros((int(nb_words*(nb_words-1)/2), 300)) #les vecteurs sont de dimension 300\n",
    "\n",
    "for i in range(nb_words) :\n",
    "    for j in range(i+1,nb_words) :\n",
    "        vect1 = model.get_vector(words_sorted[i])\n",
    "        vect2 = model.get_vector(words_sorted[j])\n",
    "        sim_mat[j-i+ nb_words*(i-1)] = (vect1-vect2).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode marche pas parce qu'elle demande trop de mémoire. Essayons une autre manière. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#on vérifie comment on peut calculer la norme d'un vecteur\n",
    "vecteur_homme = model.get_vector('man')\n",
    "norme = np.linalg.norm(vecteur_homme)\n",
    "print(norme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24065\n"
     ]
    }
   ],
   "source": [
    "#comment obtenir nb de mots contenus dans le word embedding \n",
    "nb_mots = len(model.index_to_key)\n",
    "print(nb_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogies(chosen_model, vect1, vect2, nb_candidats):\n",
    "    best_candidats = [0]*nb_candidats\n",
    "    best_scores = np.zeros(nb_candidats)\n",
    "    dir = vect1 - vect2\n",
    "    words = chosen_model.index_to_key\n",
    "    nb_mots = len(words)\n",
    "    for i in range(nb_mots): \n",
    "        for j in range(i+1, nb_mots):  \n",
    "            mot_x = words[i]\n",
    "            mot_y = words[j]\n",
    "            vect_x = chosen_model[mot_x]\n",
    "            vect_y = chosen_model[mot_y]\n",
    "            diff = vect_x - vect_y\n",
    "            if np.linalg.norm(diff) < 1 : \n",
    "                sim = cosine_similarity(dir.reshape(1, -1), diff.reshape(1, -1))\n",
    "                if sim > np.min(best_scores):\n",
    "                    index = np.argmin(best_scores)\n",
    "                    best_scores[index] = sim\n",
    "                    best_candidats[index] = (mot_x, mot_y)\n",
    "    return best_candidats, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on trouve les vecteurs associés à she et he \n",
    "vect_she = model.get_vector('she')\n",
    "vect_he = model.get_vector('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuc n'est pas présent dans le modèle Word2Vec.\n"
     ]
    }
   ],
   "source": [
    "word_to_check = 'nuc'\n",
    "\n",
    "if word_to_check in model:\n",
    "    print(f\"{word_to_check} est présent dans le modèle Word2Vec.\")\n",
    "else:\n",
    "    print(f\"{word_to_check} n'est pas présent dans le modèle Word2Vec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43manalogies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvect_she\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvect_he\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[1;32mIn[133], line 14\u001b[0m, in \u001b[0;36manalogies\u001b[1;34m(chosen_model, vect1, vect2, n)\u001b[0m\n\u001b[0;32m     12\u001b[0m vect_y \u001b[38;5;241m=\u001b[39m chosen_model\u001b[38;5;241m.\u001b[39mget_vector(mot_y)\n\u001b[0;32m     13\u001b[0m diff \u001b[38;5;241m=\u001b[39m vect_x \u001b[38;5;241m-\u001b[39m vect_y\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m : \n\u001b[0;32m     15\u001b[0m     sim \u001b[38;5;241m=\u001b[39m cosine_similarity(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), diff\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(list_score_analogy):\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\snake\\lib\\site-packages\\numpy\\linalg\\linalg.py:2511\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2509\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2511\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2512\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = list(analogies(model, vect_she, vect_he, 10))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode est trop lente (toujours pas de résultats au bout de 35mn). Essayons une autre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nos vecteurs sont normalisées, on a pour x,y des vecteurs quelconques du word embedding et a,b des vecteurs fixes (qui ici joueront le rôle de he,she) : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\cos(\\mathbf{a}-\\mathbf{b}, \\mathbf{x}-\\mathbf{y}) = (\\mathbf{a}-\\mathbf{b})^T \\cdot (\\mathbf{x}-\\mathbf{y}) = \\mathbf{a}^T \\cdot \\mathbf{x} - \\mathbf{b}^T \\cdot \\mathbf{x} - \\mathbf{a}^T \\cdot \\mathbf{y} + \\mathbf{b}^T \\cdot \\mathbf{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je veux calculer une matrice de similarité mat_sim telle que, en notant i,j les index associés respectivement aux mots x et y, on a : sim_mat(i,j) = cos(a-b,x-y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Les matrices sont identiques.\n"
     ]
    }
   ],
   "source": [
    "#rappel : X = model.vectors\n",
    "vect_in = model.get_vector('in')\n",
    "print(model.get_index('in'))\n",
    "\n",
    "# Vérifier si les matrices sont identiques\n",
    "are_identical = np.array_equal(vect_in, X[0,:])\n",
    "\n",
    "if are_identical:\n",
    "    print(\"Les matrices sont identiques.\")\n",
    "else:\n",
    "    print(\"Les matrices ne sont pas identiques.\")\n",
    "\n",
    "#donc l'index d'un mot correspond à sa ligne dans la matrice X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24065)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_1 = linear_kernel((vect_she-vect_he).reshape(1,-1), X) \n",
    "mat_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_visée = (vect_she - vect_he)/np.linalg.norm(vect_she - vect_he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_1 = linear_kernel((diff_visée).reshape(1,-1), X) \n",
    "#me donne une ligne et 24065 colonnes\n",
    "#chaque colonne correspond à la cos_sim entre un mot et le vect diff she-he\n",
    "#attention, il faut normaliser la diff pour pouvoir utilsier linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros((nb_words,nb_words))\n",
    "\n",
    "xmat_1 = np.array(mat_1[0,:])\n",
    "\n",
    "sim_mat = np.tile(xmat_1[np.newaxis,:], (nb_words,1))-np.tile(xmat_1[:,np.newaxis], (1,nb_words))\n",
    "\n",
    "#for i in range(nb_words) :\n",
    "#    if i%1000 == 0 :\n",
    "#        print(i, 'mots traités sur', nb_words)\n",
    "#    for j in range(i+1,nb_words) :\n",
    "#        sim_mat[i,j] = mat_1[0,i]-mat_1[0,j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05295623,  0.06545979,  0.06619529,  0.04707221,  0.05222073,\n",
       "       -0.08200861, -0.06141452, -0.11620951,  0.01562944,  0.09929293,\n",
       "       -0.08568612, -0.028133  ,  0.05222073,  0.05884026, -0.07759558,\n",
       "       -0.07355032,  0.03328152,  0.07722784, -0.04578507, -0.02721362,\n",
       "       -0.0342009 ,  0.0356719 , -0.09083465, -0.04817546,  0.00170085,\n",
       "        0.02794912, -0.00219502,  0.08862814,  0.04652058,  0.04817546,\n",
       "        0.06104676, -0.05185298, -0.01608913,  0.04155593, -0.06435653,\n",
       "        0.05185298, -0.09635092, -0.02500711,  0.07428582,  0.13239057,\n",
       "        0.08347961, -0.02611036, -0.03548803, -0.00638968,  0.02702974,\n",
       "        0.07759558,  0.02031828, -0.02160541, -0.00386139,  0.08016985,\n",
       "        0.04504957,  0.07097606,  0.02537486, -0.02041021, -0.07097606,\n",
       "        0.00077573, -0.03640741,  0.02592649,  0.06104676, -0.08531837,\n",
       "       -0.06693079,  0.02702974, -0.10958998, -0.1838758 , -0.0463367 ,\n",
       "        0.03990105,  0.04284306,  0.13533258,  0.04596895,  0.06545979,\n",
       "        0.0934089 , -0.03033951,  0.01700851,  0.13386159, -0.02234091,\n",
       "       -0.02234091,  0.08826038,  0.02344416, -0.07244706,  0.05001422,\n",
       "        0.00353961, -0.06031126,  0.04743996, -0.01553751, -0.04118818,\n",
       "       -0.10223494, -0.04780771,  0.06288552, -0.04817546,  0.01618107,\n",
       "        0.05810475, -0.02794912, -0.02537486, -0.1382746 , -0.05479499,\n",
       "        0.01195193,  0.07024056, -0.0463367 , -0.01071077, -0.00259725,\n",
       "        0.00836635, -0.11915152, -0.01287131,  0.00466585, -0.00657356,\n",
       "       -0.06067901, -0.01149224, -0.06619529,  0.00262023, -0.0121358 ,\n",
       "       -0.00928573,  0.07355032, -0.10517696, -0.06472428, -0.02022634,\n",
       "        0.04063655,  0.10002843,  0.08495062,  0.09120239,  0.06435653,\n",
       "       -0.00535538,  0.03364927, -0.10958998, -0.00241337, -0.08862814,\n",
       "       -0.04927871,  0.05369173, -0.07097606, -0.0228006 ,  0.09046689,\n",
       "        0.06031126, -0.07134381, -0.12209353, -0.0584725 ,  0.01599719,\n",
       "       -0.06141452,  0.002965  , -0.11841601, -0.07391807,  0.02997176,\n",
       "        0.029604  , -0.00684937,  0.07759558,  0.05111747, -0.03217826,\n",
       "        0.04780771, -0.03695904,  0.01572138, -0.12577105,  0.07024056,\n",
       "        0.0706083 ,  0.00517151,  0.04045267,  0.0395333 , -0.01838758,\n",
       "       -0.02445548, -0.0463367 , -0.00418317,  0.07244706,  0.02850075,\n",
       "        0.00919379, -0.03309764, -0.0056312 ,  0.07943434,  0.01535363,\n",
       "        0.10958998,  0.06178227,  0.00434407,  0.00344767, -0.0698728 ,\n",
       "       -0.10444146, -0.04321081, -0.03879779, -0.09855743, -0.10517696,\n",
       "       -0.01544557, -0.02041021,  0.02463936,  0.0790666 , -0.00175831,\n",
       "       -0.01700851,  0.00037924, -0.08347961,  0.06398878, -0.09782192,\n",
       "       -0.01314712, -0.00027007,  0.0812731 ,  0.06693079,  0.03364927,\n",
       "        0.01893921,  0.01792789,  0.06104676,  0.01783595, -0.08274411,\n",
       "        0.00404527, -0.013331  , -0.02555874, -0.02482323, -0.12356453,\n",
       "        0.07207932, -0.01379068,  0.0039993 , -0.02592649, -0.03328152,\n",
       "       -0.05001422, -0.01351487, -0.02234091, -0.00572313, -0.03861392,\n",
       "       -0.04082043,  0.06729854, -0.05405948,  0.01149224, -0.06215002,\n",
       "       -0.02390385,  0.02684587, -0.01599719, -0.04468182, -0.00983736,\n",
       "        0.03530415,  0.01737626,  0.01581332, -0.05920801, -0.0060679 ,\n",
       "        0.01471006, -0.00418317,  0.03125888,  0.02096184,  0.01025108,\n",
       "        0.02611036, -0.1375391 ,  0.09046689,  0.05589824, -0.03089113,\n",
       "       -0.00749294,  0.03236214, -0.00549329,  0.09267341,  0.04339469,\n",
       "       -0.0402688 , -0.02427161, -0.00684937, -0.03512028,  0.03309764,\n",
       "       -0.03824617,  0.05185298,  0.00225248, -0.00314887, -0.03328152,\n",
       "        0.05553049, -0.00960751,  0.05074972,  0.0047348 ,  0.05663374,\n",
       "       -0.02850075,  0.00367752,  0.03364927, -0.05074972,  0.00730906,\n",
       "        0.00356259,  0.01544557,  0.05369173,  0.12871306,  0.13091958,\n",
       "        0.04192368,  0.06876955, -0.028133  ,  0.03751066, -0.029604  ,\n",
       "        0.03328152,  0.04707221,  0.03659128, -0.04008492,  0.03677516,\n",
       "       -0.09855743, -0.02178928, -0.02721362, -0.04578507, -0.04321081,\n",
       "        0.09267341, -0.06215002, -0.00896395,  0.09414441,  0.00102281,\n",
       "        0.04817546, -0.08016985, -0.10811897, -0.03181051,  0.01811177,\n",
       "       -0.12724206, -0.06693079, -0.06067901,  0.04891096,  0.04615283,\n",
       "       -0.0356719 , -0.04431407, -0.03585578,  0.01089464, -0.04707221],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24065, 24065)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in for\n"
     ]
    }
   ],
   "source": [
    "mot_0 = model.index_to_key[0]\n",
    "mot_1 = model.index_to_key[1]\n",
    "print(mot_0, mot_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_0 = model.get_vector(mot_0)\n",
    "vect_1 = model.get_vector(mot_1)\n",
    "diff1 = vect_0 - vect_1\n",
    "diff0 = vect_she - vect_he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010609986\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(diff1.reshape(1, -1), diff0.reshape(1, -1))[0,0]\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0.011060394, -0.010609986)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = sim_mat[0,1]\n",
    "\n",
    "test_1 == cos_sim, test_1, cos_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n",
      "months\n",
      "-0.037064265\n",
      "0.04987895\n",
      "0.08694322\n"
     ]
    }
   ],
   "source": [
    "dir = vect_she - vect_he\n",
    "n = 9\n",
    "p = 200\n",
    "print(model.index_to_key[n])\n",
    "print(model.index_to_key[p])\n",
    "diff = model[n] - model[p]\n",
    "sim = cosine_similarity(dir.reshape(1, -1), diff.reshape(1, -1))[0,0]\n",
    "print(sim)\n",
    "print(sim_mat[n,p])\n",
    "print(sim_mat[n,p]-sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe une légère différence dans les deux calculs de la cos_similarité mais leur différence semble toujours du même signe. On peut espérer que cela n'influe pas trop sur l'ordre des valeurs de coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions si les résultats trouvés sont les mêmes que ceux de l'article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a une matrice qui nous donne les similarités, on va chercher les paires de mots associées aux plus gros coefficients, et qui vérifient que norm(x-y) < 1 (cf. condition dans l'article). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05295623,  0.06545979,  0.06619529,  0.04707221,  0.05222073,\n",
       "       -0.08200861, -0.06141452, -0.11620951,  0.01562944,  0.09929293,\n",
       "       -0.08568612, -0.028133  ,  0.05222073,  0.05884026, -0.07759558,\n",
       "       -0.07355032,  0.03328152,  0.07722784, -0.04578507, -0.02721362,\n",
       "       -0.0342009 ,  0.0356719 , -0.09083465, -0.04817546,  0.00170085,\n",
       "        0.02794912, -0.00219502,  0.08862814,  0.04652058,  0.04817546,\n",
       "        0.06104676, -0.05185298, -0.01608913,  0.04155593, -0.06435653,\n",
       "        0.05185298, -0.09635092, -0.02500711,  0.07428582,  0.13239057,\n",
       "        0.08347961, -0.02611036, -0.03548803, -0.00638968,  0.02702974,\n",
       "        0.07759558,  0.02031828, -0.02160541, -0.00386139,  0.08016985,\n",
       "        0.04504957,  0.07097606,  0.02537486, -0.02041021, -0.07097606,\n",
       "        0.00077573, -0.03640741,  0.02592649,  0.06104676, -0.08531837,\n",
       "       -0.06693079,  0.02702974, -0.10958998, -0.1838758 , -0.0463367 ,\n",
       "        0.03990105,  0.04284306,  0.13533258,  0.04596895,  0.06545979,\n",
       "        0.0934089 , -0.03033951,  0.01700851,  0.13386159, -0.02234091,\n",
       "       -0.02234091,  0.08826038,  0.02344416, -0.07244706,  0.05001422,\n",
       "        0.00353961, -0.06031126,  0.04743996, -0.01553751, -0.04118818,\n",
       "       -0.10223494, -0.04780771,  0.06288552, -0.04817546,  0.01618107,\n",
       "        0.05810475, -0.02794912, -0.02537486, -0.1382746 , -0.05479499,\n",
       "        0.01195193,  0.07024056, -0.0463367 , -0.01071077, -0.00259725,\n",
       "        0.00836635, -0.11915152, -0.01287131,  0.00466585, -0.00657356,\n",
       "       -0.06067901, -0.01149224, -0.06619529,  0.00262023, -0.0121358 ,\n",
       "       -0.00928573,  0.07355032, -0.10517696, -0.06472428, -0.02022634,\n",
       "        0.04063655,  0.10002843,  0.08495062,  0.09120239,  0.06435653,\n",
       "       -0.00535538,  0.03364927, -0.10958998, -0.00241337, -0.08862814,\n",
       "       -0.04927871,  0.05369173, -0.07097606, -0.0228006 ,  0.09046689,\n",
       "        0.06031126, -0.07134381, -0.12209353, -0.0584725 ,  0.01599719,\n",
       "       -0.06141452,  0.002965  , -0.11841601, -0.07391807,  0.02997176,\n",
       "        0.029604  , -0.00684937,  0.07759558,  0.05111747, -0.03217826,\n",
       "        0.04780771, -0.03695904,  0.01572138, -0.12577105,  0.07024056,\n",
       "        0.0706083 ,  0.00517151,  0.04045267,  0.0395333 , -0.01838758,\n",
       "       -0.02445548, -0.0463367 , -0.00418317,  0.07244706,  0.02850075,\n",
       "        0.00919379, -0.03309764, -0.0056312 ,  0.07943434,  0.01535363,\n",
       "        0.10958998,  0.06178227,  0.00434407,  0.00344767, -0.0698728 ,\n",
       "       -0.10444146, -0.04321081, -0.03879779, -0.09855743, -0.10517696,\n",
       "       -0.01544557, -0.02041021,  0.02463936,  0.0790666 , -0.00175831,\n",
       "       -0.01700851,  0.00037924, -0.08347961,  0.06398878, -0.09782192,\n",
       "       -0.01314712, -0.00027007,  0.0812731 ,  0.06693079,  0.03364927,\n",
       "        0.01893921,  0.01792789,  0.06104676,  0.01783595, -0.08274411,\n",
       "        0.00404527, -0.013331  , -0.02555874, -0.02482323, -0.12356453,\n",
       "        0.07207932, -0.01379068,  0.0039993 , -0.02592649, -0.03328152,\n",
       "       -0.05001422, -0.01351487, -0.02234091, -0.00572313, -0.03861392,\n",
       "       -0.04082043,  0.06729854, -0.05405948,  0.01149224, -0.06215002,\n",
       "       -0.02390385,  0.02684587, -0.01599719, -0.04468182, -0.00983736,\n",
       "        0.03530415,  0.01737626,  0.01581332, -0.05920801, -0.0060679 ,\n",
       "        0.01471006, -0.00418317,  0.03125888,  0.02096184,  0.01025108,\n",
       "        0.02611036, -0.1375391 ,  0.09046689,  0.05589824, -0.03089113,\n",
       "       -0.00749294,  0.03236214, -0.00549329,  0.09267341,  0.04339469,\n",
       "       -0.0402688 , -0.02427161, -0.00684937, -0.03512028,  0.03309764,\n",
       "       -0.03824617,  0.05185298,  0.00225248, -0.00314887, -0.03328152,\n",
       "        0.05553049, -0.00960751,  0.05074972,  0.0047348 ,  0.05663374,\n",
       "       -0.02850075,  0.00367752,  0.03364927, -0.05074972,  0.00730906,\n",
       "        0.00356259,  0.01544557,  0.05369173,  0.12871306,  0.13091958,\n",
       "        0.04192368,  0.06876955, -0.028133  ,  0.03751066, -0.029604  ,\n",
       "        0.03328152,  0.04707221,  0.03659128, -0.04008492,  0.03677516,\n",
       "       -0.09855743, -0.02178928, -0.02721362, -0.04578507, -0.04321081,\n",
       "        0.09267341, -0.06215002, -0.00896395,  0.09414441,  0.00102281,\n",
       "        0.04817546, -0.08016985, -0.10811897, -0.03181051,  0.01811177,\n",
       "       -0.12724206, -0.06693079, -0.06067901,  0.04891096,  0.04615283,\n",
       "       -0.0356719 , -0.04431407, -0.03585578,  0.01089464, -0.04707221],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['in']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus efficace : faire un tri des coefs puis parmi les coefs, regardez ceux qui vérifient la condition sur la norme. Pour le tri des coefs, on peut faire un tri partiel : commencer par trier les 10 000 premiers coefs et voir si parmi eux, on peut pas en avoir 10 qui vérifient la condition de norme. Regarder partial sort dans la doc --> fonction argpartition : je commence à trier les 1000 premiers, je fais mon test et si j'ai déjà les 10, je m'arrête. Algo itératif. Il faut mettre la matrice sim_mat à plat, regarder les arguments donnés par argpartition, utiliser unravel pour retrouver le i,j qui correspond à cet indice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mots traités sur 24065\n",
      "1000 mots traités sur 24065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmots traités sur\u001b[39m\u001b[38;5;124m'\u001b[39m, nb_words)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, nb_words): \n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sim_mat[i,j] \u001b[38;5;241m>\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_coefs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(model[i]\u001b[38;5;241m-\u001b[39mmodel[j]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m : \n\u001b[0;32m      9\u001b[0m             index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(best_coefs)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\snake\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2946\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[0;32m   2830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2831\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2833\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[0;32m   2834\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2944\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\snake\\lib\\site-packages\\numpy\\core\\fromnumeric.py:70\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_coefs = -np.ones(10)\n",
    "indices_best = [0]*10\n",
    "for i in range(nb_words):\n",
    "    if i%1000 == 0 :\n",
    "        print(i, 'mots traités sur', nb_words)\n",
    "    for j in range(i+1, nb_words): \n",
    "        if sim_mat[i,j] > np.min(best_coefs):\n",
    "            if np.linalg.norm(model[i]-model[j]) < 1 : \n",
    "                index = np.argmin(best_coefs)\n",
    "                best_coefs[index] = sim_mat[i,j] \n",
    "                indices_best[index] = (i,j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45190045 0.48028737 0.45140088 0.47507542 0.44541574 0.4798581\n",
      " 0.44366646 0.44685441 0.45323342 0.45971811]\n",
      "[(706, 1414), (4231, 9559), (1001, 5954), (7245, 12936), (706, 7544), (16295, 22583), (1961, 7544), (18506, 20880), (1001, 7544), (1001, 1414)]\n"
     ]
    }
   ],
   "source": [
    "print(best_coefs)\n",
    "print(indices_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother\n",
      "brother\n",
      "\n",
      "lady\n",
      "gentleman\n",
      "\n",
      "daughter\n",
      "uncle\n",
      "\n",
      "queen\n",
      "kings\n",
      "\n",
      "mother\n",
      "nephew\n",
      "\n",
      "motherhood\n",
      "fatherhood\n",
      "\n",
      "sister\n",
      "nephew\n",
      "\n",
      "gal\n",
      "fella\n",
      "\n",
      "daughter\n",
      "nephew\n",
      "\n",
      "daughter\n",
      "brother\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in indices_best :\n",
    "    analog_she = model.index_to_key[i]\n",
    "    analog_he = model.index_to_key[j]\n",
    "    print(analog_she)\n",
    "    print(analog_he)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[242], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m list_analogies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(best_coefs)): \n\u001b[1;32m----> 3\u001b[0m     mot_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     mot_2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkey_to_index[k][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     list_analogies\u001b[38;5;241m.\u001b[39mappend((mot_1, mot_2))   \n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "list_analogies = []\n",
    "for k in range(len(best_coefs)): \n",
    "    mot_1 = model.key_to_index[k][0]\n",
    "    mot_2 = model.key_to_index[k][0]\n",
    "    list_analogies.append((mot_1, mot_2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
